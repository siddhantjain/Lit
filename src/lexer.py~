import re
import sys


class Lexeme():							
	#Defines any identified Lexeme in the given string 
	#along with it's token type, value and position. 
    
    def __init__(self, type_, val, pos):				#constructor for intialising
        self.type_ = type_
        self.val = val
        self.pos = pos

    def __str__(self):					 	        #func for printing attributes 						
        return '%s(%s) at %s' % (self.type_, self.val, self.pos) #of the lexeme using print(Lexeme_name)

    
      

class Lexer():
    # The lexer class

       
    def __init__(self, regexes):
        
        #Note to Aki:
        # All the regexes are concatenated into a single regex for easy compilation. 
        # You should read more on groups in MOTW site. 
        # We create named groups for each type of regular expression 
        # A name has to be given to each group, which we auto-generate in the following code
        
        count = 1                                                       #just a counter for generation of group names         
        regex_parts = [] 
        self.group_type = {}
	

        for regex, type_ in regexes:
            groupname = 'GROUP%s' % count
            regex_parts.append('(?P<%s>%s)' % (groupname, regex))       #This basically names each group and appends it to the array(list) of regexes 
            self.group_type[groupname] = type_                           #Syntax: (P?<name>pattern)
            count += 1

        self.regex = re.compile('|'.join(regex_parts))                  #a single compilation statement for all Regexes
        self.re_ws_skip = re.compile('\S')                              #a regex to detect a substring with no white spaces    

    def input(self, buf):
     #function to initialise the input buffer for the lexer
        self.buf = buf
        self.pos = 0

    def token(self):
            # Returns the next lexeme in the form of a Lexeme object(as defined at the beginning)  
            # from the input buffer. 
            # None is returned if the end of the buffer was reached. 
            # In case of a lexing error (the current chunk of the
            # buffer matches no regex), a lexical Error is recorded with
            # the starting position of the error.
        
        if self.pos >= len(self.buf):           #end of buffer reached
            return None
        else:
            
            next_unit = self.re_ws_skip.search(self.buf, self.pos)  #neglects white space and returns next chunk of characters

            if next_unit:
                self.pos = next_unit.start()
            else:
                return None

            next_unit = self.regex.match(self.buf, self.pos)            # checks if the next unit selected is any of the lexeme 
            if next_unit:
                groupname = next_unit.lastgroup
                tok_type = self.group_type[groupname]
                tok = Lexeme(tok_type, next_unit.group(groupname), self.pos)
                self.pos = next_unit.end()
                return tok

            # if we're here next_unit was not matching any regex
            #TODO: use non-white space regex here
            initial = self.pos                  #stores the initial starting position of error
            while (self.buf[self.pos] != ' '):
                self.pos = self.pos + 1        #finds the end position of erring word (Partially Wrong) 
            tok = Lexeme('TK_ERROR',self.buf[initial:self.pos],initial)
            return tok

    
    def tokens(self):
        # Returns an iterator to the tokens found in the buffer. Iterator is a standard for OOP. You may read up on them
        
        while 1:
            tok = self.token()
            if tok is None: break 
            yield tok

    

#weird way, but this is how main can start.

if __name__ == '__main__':
    regexes = [
(r'#[^\\n]*','TK_CMNT'),
('_main','TK_MAIN'),			#placed before functions since functions have a similar starting character
(r'\$[a-zA-Z]+','TK_ID'),
(r'\"(\\.|[^"])*\"','TK_STRLIT'),
('[0-9]+\.[0-9][0-9]','TK_RNUM'), 	#Placed before following line for PoLM			
(r'\d+','TK_NUM'),
('global','TK_GLOBAL'),
('input_parameters','TK_IPP'),
('output_parameters','TK_OPP'),
('_[a-zA-Z][a-zA-Z]*','TK_FUNC'),	
(r'\[','TK_OSQ'),
(r'\]','TK_CSQ'),
(r';','TK_SCLN'),
('if','TK_IF'),
('endif','TK_EIF'),			#Placed before end to satisfy principal of longest match
('else','TK_ELSE'),
('return','TK_RET'),
('end','TK_FEND'),
('int','TK_INT'),
('float','TK_FLOAT'),
('read','TK_READ'),
('call','TK_CALL'),
('println','TK_PRINTLN'),
('print','TK_PRINT'),
(r'\(','TK_ORD'),
(r'\)','TK_CRD'),
('while','TK_WHILE'),
('endwhile','TK_EWHILE'),
(r'<\-','TK_ASSIGN'),		
('==','TK_EQ'),
('<=','TK_LTE'),		    #NOTE: Ordering is important for principal of longest match
('<','TK_LESS'),
('>=','TK_GTE'),		    #NOTE: Ordering is important for principal of longest match
('>','TK_GRTR'),
('~','TK_NOT'),
('!=','TK_NOTEQ'),
('&&','TK_AND'),
(r'\|\|','TK_OR'),
(r'\+','TK_PLUS'),
(r'\-','TK_MINUS'),
(r'\*','TK_MUL'),
(r'/','TK_DIV'),
(r'%','TK_MOD'),
(r',','TK_COMMA'),
('void','TK_VOID')

    ]

    lx = Lexer(regexes)
    inputstring = '_functionName int a; $a <- $b+$c'
    lx.input(inputstring)
    
    
    
    for tok in lx.tokens():
        test = tok.type_
        if tok.type_ == 'TK_ERROR':
            print ('PRINT IN ERROR FILE:    %s'%tok.__str__())        
        else:
            print(tok)
